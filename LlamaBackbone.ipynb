{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T__jKy3Ohg5B"
   },
   "outputs": [],
   "source": [
    "# Script adopted from Llama models/llama/llama_backbone.py\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "from keras_hub.src.api_export import keras_hub_export\n",
    "from keras_hub.src.layers.modeling.reversible_embedding import (\n",
    "    ReversibleEmbedding,\n",
    ")\n",
    "from keras_hub.src.models.backbone import Backbone\n",
    "from keras_hub.src.models.llama.llama_decoder import LlamaTransformerDecoder\n",
    "from keras_hub.src.models.llama.llama_layernorm import LlamaLayerNorm\n",
    "\n",
    "\n",
    "def _llama_kernel_initializer(stddev=0.02):\n",
    "    return keras.initializers.RandomNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "@keras_hub_export(\"keras_hub.models.LlamaBackbone\")\n",
    "class LlamaBackbone(Backbone):\n",
    "    \"\"\"\n",
    "    The Llama Transformer core architecture with hyperparameters.\n",
    "\n",
    "    This network implements a Transformer-based decoder network,\n",
    "    Llama, as described in\n",
    "    [\"Llama 7B\"](https://arxiv.org/pdf/2310.06825.pdf).\n",
    "    It includes the embedding lookups and transformer layers.\n",
    "\n",
    "    The default constructor gives a fully customizable, randomly initialized\n",
    "    Llama model with any number of layers, heads, and embedding\n",
    "    dimensions. To load preset architectures and weights, use the `from_preset`\n",
    "    constructor.\n",
    "\n",
    "    Args:\n",
    "        vocabulary_size: int. The size of the token vocabulary.\n",
    "        num_layers: int. The number of transformer layers.\n",
    "        num_query_heads : int.  The number of query attention heads for\n",
    "            each transformer.\n",
    "        hidden_dim : int.  The size of the transformer encoding and pooling\n",
    "            layers.\n",
    "        intermediate_dim : int. The output dimension of the first Dense layer in\n",
    "            a three-layer feedforward network for each transformer.\n",
    "        num_key_value_heads : int. The number of key and value attention heads\n",
    "            for each transformer.\n",
    "        rope_max_wavelength : int. The maximum angular wavelength of\n",
    "            the sine/cosine curves, for rotary embeddings. Defaults to `10000`.\n",
    "        rope_position_scaling_factor: float. The scaling factor for\n",
    "            calculation of rotary embedding. Defaults to `1.0`\n",
    "        rope_frequency_adjustment_factor: float. The scaling factor\n",
    "            used to scale the inverse frequencies.  Defaults to `None`.\n",
    "        rope_low_freq_factor: float. The low frequency scaling\n",
    "            factor. Defaults to `None`.\n",
    "        rope_high_freq_factor: float. Used for Llama3.1+. The high\n",
    "            frequency scaling factor. Defaults to `None`.\n",
    "        rope_pretraining_sequence_length: int. Used for Llama3.1+.\n",
    "            Defaults to `None`.\n",
    "        layer_norm_epsilon : float. Epsilon for the layer normalization layers\n",
    "            in the transformer decoder. Defaults to `1e-6`.\n",
    "        dtype: string or `keras.mixed_precision.DTypePolicy`. The dtype to use\n",
    "            for model computations and weights. Note that some computations,\n",
    "            such as softmax and layer normalization, will always be done at\n",
    "            float32 precision regardless of dtype.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    ```python\n",
    "    input_data = {\n",
    "        \"token_ids\": np.ones(shape=(1, 12), dtype=\"int32\"),\n",
    "        \"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]),\n",
    "    }\n",
    "\n",
    "    # Pretrained Llama decoder.\n",
    "    model = keras_hub.models.LlamaBackbone.from_preset(\"llama2_7b_en\")\n",
    "    model(input_data)\n",
    "\n",
    "    # Randomly initialized Llama decoder with custom config.\n",
    "    model = keras_hub.models.LlamaBackbone(\n",
    "        vocabulary_size=10,\n",
    "        hidden_dim=512,\n",
    "        num_layers=2,\n",
    "        num_query_heads=32,\n",
    "        num_key_value_heads=8,\n",
    "        intermediate_dim=1024,\n",
    "        layer_norm_epsilon=1e-6,\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    model(input_data)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size,\n",
    "        num_layers,\n",
    "        num_query_heads,\n",
    "        hidden_dim,\n",
    "        intermediate_dim,\n",
    "        num_key_value_heads,\n",
    "        rope_max_wavelength=10000,\n",
    "        rope_position_scaling_factor=1.0,\n",
    "        rope_frequency_adjustment_factor=None,\n",
    "        rope_low_freq_factor=None,\n",
    "        rope_high_freq_factor=None,\n",
    "        rope_pretraining_sequence_length=None,\n",
    "        layer_norm_epsilon=1e-6,\n",
    "        dropout=0,\n",
    "        dtype=None,\n",
    "        tie_word_embeddings=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # === Layers ===\n",
    "        self.token_embedding = ReversibleEmbedding(\n",
    "            input_dim=vocabulary_size,\n",
    "            output_dim=hidden_dim,\n",
    "            tie_weights=tie_word_embeddings,\n",
    "            embeddings_initializer=_llama_kernel_initializer(stddev=0.01),\n",
    "            dtype=dtype,\n",
    "            name=\"token_embedding\",\n",
    "        )\n",
    "        self.transformer_layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = LlamaTransformerDecoder(\n",
    "                intermediate_dim=intermediate_dim,\n",
    "                num_query_heads=num_query_heads,\n",
    "                num_key_value_heads=num_key_value_heads,\n",
    "                rope_max_wavelength=rope_max_wavelength,\n",
    "                rope_position_scaling_factor=rope_position_scaling_factor,\n",
    "                rope_frequency_adjustment_factor=(\n",
    "                    rope_frequency_adjustment_factor\n",
    "                ),\n",
    "                rope_low_freq_factor=rope_low_freq_factor,\n",
    "                rope_high_freq_factor=rope_high_freq_factor,\n",
    "                rope_pretraining_sequence_length=(\n",
    "                    rope_pretraining_sequence_length\n",
    "                ),\n",
    "                layer_norm_epsilon=layer_norm_epsilon,\n",
    "                activation=ops.silu,\n",
    "                kernel_initializer=_llama_kernel_initializer(stddev=0.02),\n",
    "                dropout=dropout,\n",
    "                dtype=dtype,\n",
    "                name=f\"transformer_layer_{i}\",\n",
    "            )\n",
    "            self.transformer_layers.append(layer)\n",
    "        self.layer_norm = LlamaLayerNorm(\n",
    "            epsilon=layer_norm_epsilon,\n",
    "            dtype=dtype,\n",
    "            name=\"sequence_output_layernorm\",\n",
    "        )\n",
    "\n",
    "        # === Functional Model ===\n",
    "        token_id_input = keras.Input(\n",
    "            shape=(None,), dtype=\"int32\", name=\"token_ids\"\n",
    "        )\n",
    "        padding_mask_input = keras.Input(\n",
    "            shape=(None,), dtype=\"int32\", name=\"padding_mask\"\n",
    "        )\n",
    "        x = self.token_embedding(token_id_input)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x, decoder_padding_mask=padding_mask_input)\n",
    "        sequence_output = self.layer_norm(x)\n",
    "        super().__init__(\n",
    "            inputs={\n",
    "                \"token_ids\": token_id_input,\n",
    "                \"padding_mask\": padding_mask_input,\n",
    "            },\n",
    "            outputs=sequence_output,\n",
    "            dtype=dtype,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # === Config ===\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.rope_max_wavelength = rope_max_wavelength\n",
    "        self.rope_position_scaling_factor = rope_position_scaling_factor\n",
    "        self.rope_frequency_adjustment_factor = rope_frequency_adjustment_factor\n",
    "        self.rope_low_freq_factor = rope_low_freq_factor\n",
    "        self.rope_high_freq_factor = rope_high_freq_factor\n",
    "        self.rope_pretraining_sequence_length = rope_pretraining_sequence_length\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.dropout = dropout\n",
    "        self.tie_word_embeddings = tie_word_embeddings\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"vocabulary_size\": self.vocabulary_size,\n",
    "                \"num_layers\": self.num_layers,\n",
    "                \"num_query_heads\": self.num_query_heads,\n",
    "                \"hidden_dim\": self.hidden_dim,\n",
    "                \"intermediate_dim\": self.intermediate_dim,\n",
    "                \"rope_max_wavelength\": self.rope_max_wavelength,\n",
    "                \"rope_position_scaling_factor\": (\n",
    "                    self.rope_position_scaling_factor\n",
    "                ),\n",
    "                \"rope_frequency_adjustment_factor\": (\n",
    "                    self.rope_frequency_adjustment_factor\n",
    "                ),\n",
    "                \"rope_low_freq_factor\": self.rope_low_freq_factor,\n",
    "                \"rope_high_freq_factor\": self.rope_high_freq_factor,\n",
    "                \"rope_pretraining_sequence_length\": (\n",
    "                    self.rope_pretraining_sequence_length\n",
    "                ),\n",
    "                \"num_key_value_heads\": self.num_key_value_heads,\n",
    "                \"layer_norm_epsilon\": self.layer_norm_epsilon,\n",
    "                \"dropout\": self.dropout,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    @staticmethod\n",
    "    def get_layout_map(\n",
    "        device_mesh,\n",
    "        model_parallel_dim_name=\"model\",\n",
    "        data_parallel_dim_name=\"batch\",\n",
    "    ):\n",
    "        \"\"\"Get a `keras.distribution.LayoutMap` for model parallel distribution.\n",
    "\n",
    "        The returned `LayoutMap` contains the sharding spec for the Llama\n",
    "        backbone weights, so that you can use it to distribute weights across\n",
    "        the accelerators.\n",
    "\n",
    "        Example:\n",
    "        ```\n",
    "        # Feel free to change the mesh shape to balance data and model\n",
    "        # parallelism\n",
    "        mesh = keras.distribution.DeviceMesh(\n",
    "            shape=(1, 8),\n",
    "            axis_names=('batch', 'model'),\n",
    "            devices=keras.distribution.list_devices(),\n",
    "        )\n",
    "        layout_map = LlamaBackbone.get_layout_map(\n",
    "            mesh,\n",
    "            model_parallel_dim_name=\"model\",\n",
    "        )\n",
    "\n",
    "        distribution = keras.distribution.ModelParallel(\n",
    "            layout_map=layout_map,\n",
    "            batch_dim_name='batch',\n",
    "        )\n",
    "\n",
    "        with distribution.scope():\n",
    "           llama_model = keras_hub.models.LlamaCausalLM.from_preset()\n",
    "        ```\n",
    "\n",
    "        To see how the layout map was applied, load the model then run\n",
    "        (for one decoder block):\n",
    "        ```\n",
    "        embedding_layer = llama_model.backbone.get_layer(\"token_embedding\")\n",
    "        decoder_block_1 = llama_model.backbone.get_layer('transformer_layer_0')\n",
    "        for variable in embedding_layer.weights + decoder_block_1.weights:\n",
    "            print(\n",
    "                f'{variable.path:<58}  {str(variable.shape):<16}  '\n",
    "                f'{str(variable.value.sharding.spec)}'\n",
    "            )\n",
    "        ```\n",
    "\n",
    "        Args:\n",
    "            device_mesh: The `keras.distribution.DeviceMesh` instance for\n",
    "                distribution.\n",
    "            model_parallel_dim_name: The axis name of the device mesh, where\n",
    "                the weights should be partition on.\n",
    "            data_parallel_dim_name: The axis name of the device mesh, where\n",
    "                the data should be partition on.\n",
    "        Return:\n",
    "            `keras.distribution.LayoutMap` that contains the sharding spec\n",
    "            for all the model weights.\n",
    "        \"\"\"\n",
    "        # The weight path and shape of the Llama backbone is like below\n",
    "        # token_embedding/embeddings                              (128256, 2048)\n",
    "        # repeat block for decoder\n",
    "        # transformer_layer_0/self_attention/query/kernel         (2048, 32, 64)\n",
    "        # transformer_layer_0/self_attention/key/kernel           (2048, 8, 64)\n",
    "        # transformer_layer_0/self_attention/value/kernel         (2048, 8, 64)\n",
    "        # transformer_layer_0/self_attention/attention_output/kernel\n",
    "        #                                                         (32, 64, 2048)\n",
    "        # transformer_layer_0/self_attention_layernorm/scale      (2048,)\n",
    "        # transformer_layer_0/feedforward_intermediate_dense/kernel\n",
    "        #                                                         (2048, 8192)\n",
    "        # transformer_layer_0/feedforward_gate_dense/kernel       (2048, 8192)\n",
    "        # transformer_layer_0/feedforward_output_dense/kerne      (8192, 2048)\n",
    "        # transformer_layer_0/feedforward_layernorm/scale         (2048,)\n",
    "\n",
    "        if not isinstance(device_mesh, keras.distribution.DeviceMesh):\n",
    "            raise ValueError(\n",
    "                \"Invalid device_mesh type. Expected \"\n",
    "                f\"`keras.distribution.Device`, got {type(device_mesh)}\"\n",
    "            )\n",
    "        if model_parallel_dim_name not in device_mesh.axis_names:\n",
    "            raise ValueError(\n",
    "                f\"{model_parallel_dim_name} is not found in the \"\n",
    "                f\"device_mesh.axis_names. {device_mesh.axis_name=}\"\n",
    "            )\n",
    "        if data_parallel_dim_name not in device_mesh.axis_names:\n",
    "            raise ValueError(\n",
    "                f\"{data_parallel_dim_name} is not found in the \"\n",
    "                f\"device_mesh.axis_names. {device_mesh.axis_name=}\"\n",
    "            )\n",
    "        # Note that it is possible to further config the mesh to be 3D, eg\n",
    "        # (data, seq, model). We leave it as 2D for now for simplicity.\n",
    "        data_dim = data_parallel_dim_name\n",
    "        model_dim = model_parallel_dim_name\n",
    "        # The sharding config is based on the Gemma team training config.\n",
    "        # See https://arxiv.org/abs/2403.08295\n",
    "        layout_map = keras.distribution.LayoutMap(device_mesh)\n",
    "        layout_map[\"token_embedding/embeddings\"] = (model_dim, data_dim)\n",
    "        layout_map[\n",
    "            \"transformer_layer.*self_attention.*(query|key|value).kernel\"\n",
    "        ] = (\n",
    "            model_dim,\n",
    "            data_dim,\n",
    "            None,\n",
    "        )\n",
    "        layout_map[\"transformer_layer.*attention_output.kernel\"] = (\n",
    "            model_dim,\n",
    "            None,\n",
    "            data_dim,\n",
    "        )\n",
    "        layout_map[\n",
    "            \"transformer_layer.*feedforward_intermediate_dense.kernel\"\n",
    "        ] = (\n",
    "            data_dim,\n",
    "            model_dim,\n",
    "        )\n",
    "        layout_map[\"transformer_layer.*feedforward_gate_dense.kernel\"] = (\n",
    "            data_dim,\n",
    "            model_dim,\n",
    "        )\n",
    "        layout_map[\"transformer_layer.*feedforward_output_dense.kernel\"] = (\n",
    "            model_dim,\n",
    "            data_dim,\n",
    "        )\n",
    "\n",
    "        return layout_map"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPnUcKfb8ScIQj4I5Z6INrD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
